{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc2c4f4-a236-46c6-8681-451d281eb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS_2225_u2.ipynb\n",
    "# Unit 2\n",
    "# 25 May 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19e7f9-c0dc-40b5-90db-83bca3dec8fd",
   "metadata": {},
   "source": [
    "## **Data smoothing**\n",
    "\n",
    "is a technique in **data transformation** used to reduce noise, fluctuations, or inconsistencies in a dataset, making patterns more detectable and analysis more reliable. It helps improve the quality of data by eliminating sudden changes or outliers that might interfere with meaningful insights.\n",
    "\n",
    "### Why Use Data Smoothing?\n",
    "- **Reduces noise** ‚Üí Helps remove random variations or errors.\n",
    "- **Enhances trends** ‚Üí Makes it easier to identify patterns in data.\n",
    "- **Improves predictive models** ‚Üí Helps algorithms make better decisions.\n",
    "- **Handles outliers** ‚Üí Smooths irregular data points to prevent extreme variations.\n",
    "\n",
    "### Common Data Smoothing Techniques:\n",
    "1. **Moving Average (Rolling Average)**\n",
    "   - Computes the average of a window of consecutive values.\n",
    "   - Example: Stock price trends ‚Üí averaging over the last 7 days.\n",
    "\n",
    "2. **Weighted Moving Average**\n",
    "   - Similar to moving average but gives higher importance to recent values.\n",
    "\n",
    "3. **Exponential Smoothing**\n",
    "   - Assigns exponentially decreasing weights to older observations.\n",
    "   - Useful in **time series forecasting**.\n",
    "\n",
    "4. **Bin Smoothing**\n",
    "   - Groups similar values into bins, then replaces values with the bin average.\n",
    "   - Common in data preprocessing.\n",
    "\n",
    "5. **Regression Smoothing**\n",
    "   - Fits a regression model to the data, removing unnecessary fluctuations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129a724-12cf-45b9-945d-103230cbf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Values': [10, 12, 20, 25, 18, 15, 30, 40, 35]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply moving average with a window of 3\n",
    "df['Smoothed'] = df['Values'].rolling(window=3).mean()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503a171-17ba-4989-a7cf-e3edb7c770ce",
   "metadata": {},
   "source": [
    "### **Binning**\n",
    "\n",
    "is a data transformation technique used to group continuous numerical values into discrete categories or ranges, known as **bins**. It helps reduce noise, improve data interpretation, and enhance the performance of machine learning models.\n",
    "\n",
    "### Why Use Binning?\n",
    "- **Smooths Data**: Helps reduce the impact of outliers and noise.\n",
    "- **Improves Interpretability**: Converts complex numerical data into simpler categories.\n",
    "- **Enhances Model Performance**: Reduces overfitting by making data more structured.\n",
    "- **Prepares Data for Algorithms**: Some machine learning models perform better with binned data.\n",
    "\n",
    "### Types of Binning:\n",
    "1. **Equal Width Binning**: Divides the data into bins of equal size.\n",
    "   - Example: If values range from 0 to 100, and we create 4 bins, they could be: **0-25**, **26-50**, **51-75**, **76-100**.\n",
    "\n",
    "2. **Equal Frequency Binning**: Each bin contains approximately the same number of observations.\n",
    "   - Example: If we have 100 values, we can divide them into 4 bins, each containing **25 values**.\n",
    "\n",
    "3. **Custom Binning**: Manually defining bin ranges based on domain knowledge.\n",
    "   - Example: Age groups ‚Üí **0-12 (child)**, **13-19 (teen)**, **20-59 (adult)**, **60+ (senior)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cf0cb5-f33e-4bfb-a2ac-4bbadb3fa15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>Teen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Age Group\n",
       "0    5     Child\n",
       "1   16      Teen\n",
       "2   29     Adult\n",
       "3   45     Adult\n",
       "4   67    Senior\n",
       "5   80    Senior"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Age': [5, 16, 29, 45, 67, 80]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 12, 19, 59, 100]\n",
    "labels = ['Child', 'Teen', 'Adult', 'Senior']\n",
    "\n",
    "# Apply binning\n",
    "df['Age Group'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
    "#print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8548870-39b4-4b3d-90f7-655bb6381bc9",
   "metadata": {},
   "source": [
    "### **Attribute construction** (also known as **feature construction**) \n",
    "\n",
    "is a data transformation technique where new attributes (features) are created from existing data to improve the **quality**, **accuracy**, and **predictive power** of a dataset.\n",
    "\n",
    "### Why Use Attribute Construction?\n",
    "- **Enhances model performance** ‚Üí Helps machine learning models capture better patterns.\n",
    "- **Simplifies complex data** ‚Üí Converts raw data into more meaningful features.\n",
    "- **Improves interpretability** ‚Üí Makes data more useful for analysis.\n",
    "- **Reduces dimensionality** ‚Üí Sometimes leads to fewer but more informative features.\n",
    "\n",
    "### Techniques for Attribute Construction:\n",
    "1. **Mathematical Transformations**  \n",
    "   - Example: Creating a `BMI` attribute from `Height` and `Weight` using the formula:  \n",
    "     $$BMI = \\frac{\\text{Weight (kg)}}{\\text{Height (m)}^2}$$\n",
    "\n",
    "2. **Aggregations**  \n",
    "   - Example: Turning hourly temperature readings into daily averages.\n",
    "\n",
    "3. **Binning (Discretization)**  \n",
    "   - Example: Converting `Age` into groups like `Child`, `Adult`, and `Senior`.\n",
    "\n",
    "4. **Domain-Specific Rules**  \n",
    "   - Example: Creating a new attribute `Risk_Level` based on financial data.\n",
    "\n",
    "5. **Text Processing Features**  \n",
    "   - Example: Extracting the length of customer reviews as an attribute.\n",
    "\n",
    "6. **Date & Time Transformations**  \n",
    "   - Example: Splitting a timestamp into separate attributes like `Year`, `Month`, `Day`, or `Weekday`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a104ed17-3bbe-41f2-8c85-750c1b9317d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.75</td>\n",
       "      <td>68</td>\n",
       "      <td>22.204082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.62</td>\n",
       "      <td>55</td>\n",
       "      <td>20.957171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.80</td>\n",
       "      <td>78</td>\n",
       "      <td>24.074074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height  Weight        BMI\n",
       "0    1.75      68  22.204082\n",
       "1    1.62      55  20.957171\n",
       "2    1.80      78  24.074074"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Height': [1.75, 1.62, 1.80], 'Weight': [68, 55, 78]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create BMI attribute\n",
    "df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4df774-fdbf-45c3-9e00-40a005ec06f1",
   "metadata": {},
   "source": [
    "### Min-Max Scaling\n",
    "\n",
    "Min-Max Scaling is a normalization technique used in data preprocessing to rescale numerical values into a fixed range, typically [0,1] or [-1,1]. It ensures that all data points are on a uniform scale without distorting relative relationships.\n",
    "\n",
    "Formula for Min-Max Scaling\n",
    "Given a value X in the dataset, Min-Max scaling transforms it as:\n",
    "\n",
    "$$X_{new} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- X_min = Minimum value in the dataset\n",
    "- X_max = Maximum value in the dataset\n",
    "- X_new = Scaled value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b98a4e-6122-4abe-9f4b-8a38d955af60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42857143],\n",
       "       [0.14285714],\n",
       "       [1.        ],\n",
       "       [0.57142857],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[50], [30], [90], [60], [20]])\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# print(scaled_data)\n",
    "scaled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6570d-e317-4564-9323-044b5396d7d2",
   "metadata": {},
   "source": [
    "### **Z-score normalization** (also called **standardization**) \n",
    "\n",
    "is a **data transformation** technique that rescales numerical values by centering them around the **mean** and adjusting for **standard deviation**. This helps ensure all features have the same scale, making data more suitable for machine learning.\n",
    "\n",
    "### **Formula for Z-Score Normalization**\n",
    "\n",
    "$$[X_{new} = \\frac{X - \\mu}{\\sigma}]$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "\n",
    "$$X \\rightarrow \\text{Original value in the dataset}$$\n",
    "\n",
    "$$\\mu \\rightarrow \\text{Mean of the dataset}$$\n",
    "\n",
    "$$\\sigma \\rightarrow \\text{Standard deviation of the dataset}$$\n",
    "$$X_{new} \\rightarrow \\text{Transformed (normalized) value}$$\n",
    "\n",
    "\n",
    "### **Why Use Z-Score Normalization?**\n",
    "‚úÖ **Centers data around 0** ‚Üí Mean becomes **0**, and variance becomes **1**  \n",
    "‚úÖ **Handles outliers better than Min-Max scaling**  \n",
    "‚úÖ **Useful for algorithms like PCA, K-Means, Linear Regression, SVM**  \n",
    "‚úÖ **Removes units** ‚Üí Allows fair comparison across different scales  \n",
    "\n",
    "\n",
    "\n",
    "Now, the values have **mean ‚âà 0** and **standard deviation ‚âà 1**, making them standardized.\n",
    "\n",
    "### **Comparison with Min-Max Scaling**\n",
    "| **Method**         | **Range**       | **Handles Outliers?** | **Use Case** |\n",
    "|--------------------|----------------|----------------------|--------------|\n",
    "| **Min-Max Scaling** | `[0,1]` or `[-1,1]` | ‚ùå No | When data is evenly distributed |\n",
    "| **Z-Score Scaling** | No fixed range | ‚úÖ Yes | When data has extreme variations |\n",
    "| **Robust Scaling** | No fixed range | ‚úÖ Yes | When handling heavy outliers |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc995b76-efcc-4415-9c32-40b335303068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [-0.81649658],\n",
       "       [ 1.63299316],\n",
       "       [ 0.40824829],\n",
       "       [-1.22474487]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[50], [30], [90], [60], [20]])\n",
    "\n",
    "# Apply Z-score normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "#print(scaled_data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafd143-f19b-4ac8-b098-7b02c61f8575",
   "metadata": {},
   "source": [
    " ## **Sample Space** and **Events** \n",
    "\n",
    "### **1. Sample Space (Œ©)**\n",
    "The **Sample Space** is the set of all possible outcomes of an experiment.\n",
    "\n",
    "#### **Example in Data Science**\n",
    "Imagine a model predicting customer satisfaction as **Positive, Neutral, or Negative**. The sample space is:\n",
    "$$\n",
    "[\n",
    "\\Omega = \\{\\text{Positive, Neutral, Negative}\\}\n",
    "]\n",
    "$$\n",
    "Each outcome represents a possible result of an observation.\n",
    "\n",
    "#### **Example in Dice Rolls**\n",
    "For rolling a six-sided die:\n",
    "$$\n",
    "[\n",
    "\\Omega = \\{1, 2, 3, 4, 5, 6\\}\n",
    "]\n",
    "$$\n",
    "Every possible outcome belongs to the sample space.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Events (E)**\n",
    "An **Event** is a subset of the sample space‚Äîit represents specific outcomes of interest.\n",
    "\n",
    "#### **Example in Data Science**\n",
    "- Let **Event \\( E \\)** be users who rated a product **\"Positive\"**  \n",
    "\n",
    "$$\n",
    "[\n",
    "  E = \\{\\text{Positive}\\}\n",
    "]\n",
    "$$\n",
    "- Or, let **Event \\( F \\)** be users **not satisfied**\n",
    "$$\n",
    "[\n",
    "  F = \\{\\text{Neutral, Negative}\\}\n",
    "]\n",
    "$$\n",
    "Events help analyze **probabilities of real-world occurrences**.\n",
    "\n",
    "#### **Example in Dice Rolls**\n",
    "- **Event \\( A \\)**: Rolling an even number\n",
    "\n",
    "  [\n",
    "  A = \\{2, 4, 6\\}\n",
    "  \\]\n",
    "- **Event \\( B \\)**: Rolling a number less than 4  \n",
    "  \\[\n",
    "  B = \\{1, 2, 3\\}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Probability & Events in Data Science**\n",
    "Probabilities are assigned to events based on observations or assumptions:\n",
    "\n",
    "$$\n",
    "[\n",
    "P(E) = \\frac{\\text{Favorable Outcomes}}{\\text{Total Outcomes}}\n",
    "]\n",
    "$$\n",
    "\n",
    "For instance, if **60% of customers rate a product positively**, then:\n",
    "\n",
    "\\[\n",
    "P(E) = 0.6\n",
    "\\]\n",
    "\n",
    "Events help in **decision-making, risk analysis, and predictive modeling**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb09eb-cfa5-4dc6-851f-7b4e64ca1947",
   "metadata": {},
   "source": [
    "### **Probability in Data Science**\n",
    "**Probability** is the foundation of data science, used to quantify uncertainty and make predictions based on data patterns. It plays a crucial role in **machine learning, statistics, and decision-making models**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Probability Matters in Data Science?**\n",
    "‚úÖ **Predictive Modeling** ‚Üí Helps estimate future outcomes (e.g., will a customer buy a product?).  \n",
    "‚úÖ **Classification & Clustering** ‚Üí Used in algorithms like **Na√Øve Bayes**, **Logistic Regression**, and **Hidden Markov Models**.  \n",
    "‚úÖ **Statistical Inference** ‚Üí Determines likelihood of events, crucial for **A/B Testing** and hypothesis testing.  \n",
    "‚úÖ **Risk Assessment** ‚Üí Helps in fraud detection, disease prediction, and financial analysis.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Core Probability Concepts in Data Science**\n",
    "1Ô∏è‚É£ **Sample Space (Œ©):**  \n",
    "   - The set of all possible outcomes.  \n",
    "   - Example: Rolling a die ‚Üí `Œ© = {1, 2, 3, 4, 5, 6}`  \n",
    "\n",
    "2Ô∏è‚É£ **Events (E):**  \n",
    "   - A subset of the sample space.  \n",
    "   - Example: Rolling an even number ‚Üí `E = {2, 4, 6}`  \n",
    "\n",
    "3Ô∏è‚É£ **Probability of an Event (P(E)):**  \n",
    "   - Likelihood of an event occurring:\n",
    "$$\n",
    "   [\n",
    "   P(E) = \\frac{\\text{Favorable Outcomes}}{\\text{Total Outcomes}}\n",
    "   ]\n",
    "$$\n",
    "   - Example: Probability of rolling a **2** ‚Üí `P(2) = 1/6`\n",
    "\n",
    "4Ô∏è‚É£ **Conditional Probability:**  \n",
    "   - Probability of event **A** happening **given** event **B** has already occurred.  \n",
    "$$   \n",
    "[\n",
    "   P(A | B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "] \n",
    "$$\n",
    "   - Example: Probability that a customer **buys a product**, given they **visited the website**.\n",
    "\n",
    "5Ô∏è‚É£ **Bayes‚Äô Theorem:**  \n",
    "   - Used in **classification algorithms** and **spam detection**.\n",
    "$$\n",
    "[\n",
    "   P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
    "]\n",
    "$$\n",
    "\n",
    "6Ô∏è‚É£ **Probability Distributions:**  \n",
    "   - **Normal Distribution** ‚Üí Data follows a bell curve (used in ML).  \n",
    "   - **Binomial Distribution** ‚Üí Probability of success/failure over multiple trials.  \n",
    "   - **Poisson Distribution** ‚Üí Used for rare event occurrences (e.g., network failures).\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-World Applications in Data Science**\n",
    "üöÄ **Spam Filtering** ‚Üí Classifies emails based on word probability (Na√Øve Bayes).  \n",
    "üìà **Stock Market Predictions** ‚Üí Uses probabilistic models to estimate future trends.  \n",
    "üîé **Fraud Detection** ‚Üí Probability helps detect unusual transactions.  \n",
    "ü©∫ **Medical Diagnosis** ‚Üí Identifies disease likelihood based on symptoms.\n",
    "\n",
    "Probability helps **extract meaningful insights from data** and drives intelligent decision-making in AI and ML models. Would you like examples with Python code? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396ebf7-a0e6-47b9-8f37-48cedfd55808",
   "metadata": {},
   "source": [
    "### **Understanding Probability Distributions in Data Science**\n",
    "A **probability distribution** defines how values of a random variable are distributed‚Äîit describes the likelihood of different possible outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Probability Distributions**\n",
    "#### 1Ô∏è‚É£ **Discrete Probability Distributions** (Finite Outcomes)\n",
    "Used when the variable can take only specific values (e.g., **counts, categories**).\n",
    "‚úÖ **Binomial Distribution** ‚Üí Models the probability of success/failure over multiple trials  \n",
    "‚úÖ **Poisson Distribution** ‚Üí Used for rare events (e.g., the number of website crashes per day)\n",
    "\n",
    "#### 2Ô∏è‚É£ **Continuous Probability Distributions** (Infinite Outcomes)\n",
    "Used when the variable can take any value within a range (e.g., **height, temperature**).\n",
    "‚úÖ **Normal Distribution** (Gaussian) ‚Üí Bell-shaped curve; real-world phenomena often follow this  \n",
    "‚úÖ **Exponential Distribution** ‚Üí Describes waiting times between events  \n",
    "\n",
    "---\n",
    "\n",
    "### **Common Distributions in Data Science**\n",
    "| **Distribution**      | **Type**     | **Use Case** |\n",
    "|----------------------|-------------|--------------|\n",
    "| **Normal (Gaussian)** | Continuous | Stock price prediction, IQ scores |\n",
    "| **Binomial**         | Discrete    | Coin tosses, survey responses |\n",
    "| **Poisson**         | Discrete    | Modeling rare events like fraud detection |\n",
    "| **Exponential**      | Continuous | Waiting times for a customer service call |\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-World Example: Normal Distribution**\n",
    "Many natural phenomena follow a **Normal Distribution**, which looks like a symmetric bell curve:\n",
    "$$\n",
    "[\n",
    "X \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "]\n",
    "$$\n",
    "Where:\n",
    "- **\\( \\mu \\)** = Mean (center of distribution)\n",
    "- **\\( \\sigma^2 \\)** = Variance (spread of data)\n",
    "\n",
    "Example in Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate data\n",
    "data = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6)\n",
    "\n",
    "# Plot normal curve\n",
    "x = np.linspace(-4, 4, 100)\n",
    "plt.plot(x, norm.pdf(x, 0, 1))\n",
    "plt.title(\"Normal Distribution\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Would you like a deeper dive into a specific distribution? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faad52d-20ac-4ce3-aec5-8848c8dc3988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
